{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Q1 What is Simple Linear Regression?\n",
        "\n",
        "It is a supervised machine learning algorithm it use to determine strenth & caracterstics of relationship btw 1 independent and another dependent variabl\n",
        "\n",
        "#Q2 What are key assumption of SLR?\n",
        "\n",
        "1. linearity\n",
        "2. indepenency of error\n",
        "3. equal variance (homoscedasticity)\n",
        "\n",
        "#Q3 What does the coefficient m represent in the equation Y=mX+c?\n",
        "\n",
        "M represent slope of the linear relationship\n",
        "\n",
        "#4 What does intercept c represent in y=mx+c?\n",
        "\n",
        "it represent the value of y when the x is zero\n",
        "\n",
        "#5 how we calculate m in slr?\n",
        "find mean of x and y\n",
        "compute deviation\n",
        "multiple deviation for each pair\n",
        "sum them and square the deiation and then divide by n. of terms\n",
        "\n",
        "#6 What is the purpose of the least squares method in Simple Linear Regression\n",
        " to minimise the error of slr\n",
        "\n",
        "#7How is the coefficient of determination (R¬≤) interpreted in Simple Linear Regression\n",
        "(R¬≤)= 1-residual sum of square/total sum of square\n",
        "Interpretation\n",
        "\n",
        "ùëÖ2=1 ‚Üí Perfect fit (the regression line passes through all points).\n",
        "\n",
        "ùëÖ2=0 ‚Üí Model explains none of the variability (just using the mean of y would be equally good).\n",
        "\n",
        "ùëÖ2=0.75 ‚Üí 75% of the variance in y is explained by ùë•\n",
        "x; 25% remains unexplained.\n",
        "\n",
        "#8 What is MLR?\n",
        "when we need multiple independent variable to explain the dependennt variable  that type of relationship is called multiple linear regression  \n",
        "\n",
        "#9 What is main diff btw slr and mlr?\n",
        "slr use to establish relationship btw 2 vriables whereas\n",
        "mlr use to establish relationship btw multiple variables\n",
        "\n",
        "#10 what is key assumption of mlr ?\n",
        " linearity\n",
        " indepeendece f error\n",
        " normality of error\n",
        " homoscedasticity\n",
        "\n",
        "# Q11 What is heteroscedasticity, and how does it affect the results of Multiple Linear Regression model\n",
        "\n",
        "It means error is not constant across all levels of predicted value due to which it create regression coefficient are still unbiased , standard error become wrong\n",
        "\n",
        "# Q12 How can you improve a Multiple Linear Regression model with high multicollinearity\n",
        "MLR happen when 2 or more independent variable in a MLR model are highly correlated with each other\n",
        "| **Approach**             | **When to Use**                                         |\n",
        "| ------------------------ | ------------------------------------------------------- |\n",
        "| Remove one variable      | When two predictors are highly redundant                  |\n",
        "| Combine variables        | When predictors measure the same thing                      |\n",
        "| PCA / PLS                | When you want to keep information but avoid correlation                |\n",
        "| Ridge / Lasso regression | When you want automatic regularization             |\n",
        "| Collect more data        | When feasible, improves coefficient stability                  |\n",
        "\n",
        "#Q 13 What are some common techniques for transforming categorical variables for use in regression models?\n",
        "\n",
        "one hot enccoding\n",
        "label encoding\n",
        "target/mean encoding\n",
        "\n",
        "#Q14 What is the role of interaction terms in Multiple Linear Regression?\n",
        "It allow the model to capture combine effect of two or more independent varible on dependent variable\n",
        "standard  y=Œ≤0‚Äã+Œ≤1‚ÄãX1‚Äã+Œ≤2‚ÄãX2‚Äã+Œµ\n",
        "interaction x1*x2 y=Œ≤0‚Äã+Œ≤1‚ÄãX1‚Äã+Œ≤2‚ÄãX2‚Äã+Œ≤3‚Äã(X1‚ÄãX2‚Äã)+Œµ\n",
        "\n",
        "#Q15  How can the interpretation of intercept differ between Simple and Multiple Linear Regression?\n",
        "\n",
        "In slr the intercept is based on one independent variable where as in mlr intercept based oon multiple independent variable\n",
        "\n",
        "#Q16 What is significamce of slope in regression analysis , and how does it effect prediction?\n",
        "It can explain the relationship btw the dependent and indepedent variable give exact fraction or number of time it can multiple with indepnendent varibl to get dependent value\n",
        "\n",
        "#q17 How does the intercept in a regression model provide context for the relationship between variables\n",
        "IT REPRESENT EXPECTED VALUE F Y WHEN ALL X VALUES ARE 0 , ENSURE ERROR TO MINIMISED\n",
        "\n",
        "#Q18 What are the limitations of using R¬≤ as a sole measure of model performance?\n",
        "Always inc with more predictors , does not indicate casuality , high r^2 does not mean god predictor\n",
        "#q19 How would you interpret a large standard error for a regression coefficient?\n",
        "standarand error measure how much a coefficent estimate might vary across diff samples large se mean unsatble and no significant  couse multicolinearity\n",
        "\n",
        "#Q20 How can heteroscedasticity be identified in residual plots, and why is it important to address it\n",
        "IN MLR  model heteroscedasticity mean error s not constatnt across all leel of independent variabls we can detetct it using residual(y-axis) vs fited value plot (x-axis)\n",
        "\n",
        "#21 What does it mean if a Multiple Linear Regression model has a high R¬≤ but low adjusted R¬≤\n",
        "\n",
        "R¬≤ inc when we add more predictor even if those predictor are not useful , and in adjusted R¬≤ for the number of preedictor in model it can decrease if we add predictor that do no explain enough variance\n",
        "\n",
        "\n",
        "#22 Why is it important to scale variables in Multiple Linear Regression\n",
        "\n",
        "scaling ensure fair treatment of variable , numerical stability an meaniingful interpretetion of coefficent\n",
        "\n",
        "# what is polynomial regression ?\n",
        "it is a type f regression analysis where relationship btw independent and dependent variable in model as an nth degreee polynomial\n",
        "\n",
        "#24How Does Polynomial Regression Differ from Linear Regression?\n",
        "poly capture non linear patten it is more flexible curve type shape where as linear regression capture linear trend it is less flexible\n",
        "\n",
        "# 25. When is Polynomial Regression Used?\n",
        "it uses when\n",
        "* data shows curve or non linear trend  tht a straight line can not captre like population growth , projection etc\n",
        "\n",
        "#26 General Equation for Polynomial Regression\n",
        "y=Œ≤0‚Äã+Œ≤1‚ÄãX‚Äã+Œ≤2‚ÄãX^2‚Äã+Œ≤3‚Äãx^3........+Œ≤3‚Äãx^n+Œµ\n",
        " where Œ≤ are the cofficent and Œµ is error\n",
        "\n",
        "#27 Can Polynomial Regression Be Applied to Multiple Variables?\n",
        "yes\n",
        "\n",
        "#28 What are the limitations of polynomial regression\n",
        "overfitting , prediction outside the training range can become extreme\n",
        "cofficients ar harder to explain\n",
        "\n",
        "#29 What methods can be used to evaluate model fit when selecting the degree of a polynomial\n",
        "R^2 / Adjusted R^2 , visual inspection cross validation\n",
        "\n",
        "#30 Why Visualization Is Important in Polynomial Regression\n",
        "\n",
        "it helps to see if the curve is underfitting or overfitting.\n",
        "Allows you to check if a higher degree really captures a better trend.\n",
        "Makes model interpretation more intuitive (curved vs. straight line).\n",
        "\n",
        "#31 How is polynomial regression implemented in Python?\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "# Sample data\n",
        "X = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)\n",
        "y = np.array([2, 5, 7, 8, 13])\n",
        "\n",
        "# Create polynomial features (degree = 2)\n",
        "poly = PolynomialFeatures(degree=2)\n",
        "X_poly = poly.fit_transform(X)\n",
        "\n",
        "# Fit linear regression on transformed data\n",
        "model = LinearRegression()\n",
        "model.fit(X_poly, y)\n",
        "\n",
        "# Predictions\n",
        "y_pred = model.predict(X_poly)\n",
        "\n",
        "# Plot\n",
        "plt.scatter(X, y, color='blue', label=\"Data Points\")\n",
        "plt.plot(X, y_pred, color='red', label=\"Polynomial Regression Curve\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Print coefficients\n",
        "print(\"Coefficients:\", model.coef_)\n",
        "print(\"Intercept:\", model.intercept_)"
      ],
      "metadata": {
        "id": "Z3ufGIaCub8r"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Z6Cya2VuSMy"
      },
      "outputs": [],
      "source": []
    }
  ]
}